\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

\newcommand{\ub}{\boldsymbol{u}}
\newcommand{\vb}{\boldsymbol{v}}

\begin{document}

\begin{center}
{\Large Reinforcement Learning David Silver - Lecture 6 Notes: Value Function Approximation}

\begin{tabular}{rl}
Name: & Eli Andrew
\end{tabular}
\end{center}

  \begin{itemize}
    \item Estimate value function with function approximation:
    \begin{gather*}
      \hat{v}(s, \textbf{w}) \approx v_\pi(s) \\
      \hat{q}(s, a, \textbf{w}) \approx q_\pi(s,a)
    \end{gather*}
    \item Update parameter $\textbf{w}$ using MC and TD methods
    \item \textbf{Value Function Approximation with SGD}
    \begin{itemize}
      \item Goal: find parameter vector $\textbf{w}$ minimizing MSE between
      approximate value function $\hat{v}(s, \textbf{w})$ and true value function $v_\pi(s)$
      \begin{gather*}
        J(\textbf{w}) = E_\pi[(v_\pi(S) - \hat{v}(S, \textbf{w}))^2]
      \end{gather*}
      \item Gradient descent finds local minimum:
      \begin{gather*}
        \nabla\textbf{w} = -\frac{1}{2}\alpha\nabla_{\textbf{w}}J(\textbf{w}) \\
        = \alpha E_\pi[(v_\pi(S) - \hat{v}(S, \textbf{w}))\nabla_{\textbf{w}}\hat{v}(S, \textbf{w})]
      \end{gather*}
      \item SGD \textit{samples} the gradient (removes expectation):
      \begin{gather*}
        \Delta\textbf{w} = \alpha(v_\pi(S) - \hat{v}(S, \textbf{w}))\nabla_{\textbf{w}}\hat{v}(S, \textbf{w})
      \end{gather*}
      \item $\alpha$ gives step size: how much to adjust current estimate towards the true value
      \item $v_\pi(S) - \hat{v}(S, \textbf{w})$ gives the total error of our current estimate
      \item $\nabla_{\textbf{w}}\hat{v}(S, \textbf{w})$ gives how much each component of $\textbf{w}$ contributed to the overall error
    \end{itemize}
    \item State representation is done using \textit{feature vectors} 
    \begin{gather*}
      \textbf{x}(S) = (\textbf{x}_1(S), \dots, \textbf{x}_n(S))
    \end{gather*}
    \item \textbf{Linear Value Function Approximation}
    \begin{itemize}
      \item Represent value function by linear combination of features:
      \begin{gather*}
        \hat{v}(S, \textbf{w}) = \textbf{x}(S)^\top \textbf{w} = \sum_{j = 1}^n \textbf{x}_j(S) * \textbf{w}_j
      \end{gather*}
      \item Objective function is quadratic in parameters $\textbf{w}$:
      \begin{gather*}
        J(\textbf{w}) = E_\pi[(v_\pi(S) - \textbf{x}(S)^\top\textbf{w})^2]
      \end{gather*}
      \item SGD converges on global optimum using update rule:
      \begin{gather*}
        \nabla_{\textbf{w}}\hat{v}(S, \textbf{w}) = \textbf{x}(S) \\
        \Delta \textbf{w} = \alpha (v_\pi(S) - \hat{v}(S, \textbf{w}))\textbf{x}(S)
      \end{gather*}
    \end{itemize}
    \item \textbf{Table lookup features} (special case of linear function approximation)
    \begin{itemize}
      \item Table lookup features just indicate if we're in a given state
      \begin{gather*}
        x^{table}(S) = (\textbf{1}(S = s_1), \dots, \textbf{1}(S = s_n))
      \end{gather*}
      \item Essentially a one-hot encoded vector with length equal to the number of states
      \item When we multiple this feature vector by our weights $\textbf{w}$ we are just selecting
      the weight at the same entry as our active state ($s_n$ selects $w_n$)
      \item In the previous methods, this weight that we're selecting is the current estimate of the value of that state
    \end{itemize}
    \item \textbf{Incremental Prediction Algorithms}
    \begin{itemize}
      \item There is no \textit{true} value function ever actually available
      \item So, we must use our current target $v_\pi(s)$ instead
      \item This gives our $\Delta \textbf{w}$ as:
      \begin{gather*}
        \Delta \textbf{w} = \alpha (v_{target}(S) - \hat{v}(S, \textbf{w}))\textbf{x}(S) 
      \end{gather*}
      \item Where the $v_{target}(S)$ is defined by the algorithm we're using:
      \begin{itemize}
        \item Monte-Carlo: $G_t$
        \item TD(0): $R_{t+1} + \gamma \hat{v}(S_{t+1}, \textbf{w})$
        \item TD($\lambda$): $G^\lambda_t$
      \end{itemize}
    \end{itemize}
    \item \textbf{Learning with Function Approximation}
    \begin{itemize}
      \item All algorithms (MC, TD, etc.) are used to generate training data that can then
      be used for updating our $\textbf{w}$ to better predict our $v_{target}(S)$
      \item Monte Carlo samples training data and performs updates through:
      \begin{gather*}
        (S_1, G_1), (S_2, G_2), \dots, (S_T, G_T) \\
        \Delta \textbf{w} = \alpha (G_t - \hat{v}(S, \textbf{w}))\textbf{x}(S)
      \end{gather*}
      \item TD(0) does this through:
      \begin{gather*}
        (S_1, R_2 + \gamma \hat{v}(S_2, \textbf{w})), \dots, (S_{T-1}, R_T) \\
        \Delta \textbf{w} = \alpha (R_{t+1} + \gamma \hat{v}(S_{t + 2}))\textbf{x}(S) \\
        = \alpha \delta \textbf{x}(S)
      \end{gather*}
      \item And TD($\lambda$) does this through:
      \begin{gather*}
        (S_1, G_1^\lambda), \dots, (S_{T-1}, G_{T-1}^\lambda) \\
        \delta_t = R_{t+1} + \gamma \hat{v}(S_{t+1}, \textbf{w}) - \hat{v}(S_t, \textbf{w}) \\
        E_t = \gamma \lambda E_{t-1} + \textbf{x}(S_t) \\
        \Delta \textbf{w} = \alpha \delta_t E_t
      \end{gather*}
      \item The eligibility trace here is for features now (instead of states)
      \item In the incremental case we perform these updates to our weights on every single
      step of the epsiode (as determined by the algorithm we're using). For example, for Monte-Carlo
      it's one update at the end of every episode and for TD(0) its at every step.
    \end{itemize}
    \item \textbf{Control with Function Approximation}
    \begin{itemize}
      \item Action-Value Function  Approximation
      \begin{itemize}
        \item Approximate the action-value function
        \begin{gather*}
          \hat{q}(S, A, \textbf{w}) \approx q_\pi(S, A) \\
        \end{gather*}
        \item Minimize MSE between $\hat{q}$ and $q$
        \begin{gather*}
          J(\textbf(w)) = E_\pi[(q_\pi(S, A) - \hat{q}(S, A, \textbf{w}))^2]
        \end{gather*}
        \item Use SGD to find local minimum
        \begin{gather*}
          -\frac{1}{2}\nabla J(\textbf{w}) = (q_\pi(S, A) - \hat{q}(S, A, \textbf{w}))\nabla_{\textbf{w}}\hat{q}(S, A, \textbf{w}) \\
          \Delta_{\textbf{w}} = \alpha (q_\pi(S, A) - \hat{q}(S, A, \textbf{w}))\nabla_{\textbf{w}}\hat{q}(S, A, \textbf{w})
        \end{gather*}
        \item Feature vector $\textbf{x}$ is now a function of both $S$ and $A$
      \end{itemize}
    \end{itemize}
    \item \textbf{Convergence of Prediction Algorithms} \\ \\
    \begin{tabular}{c | c | c | c}
    \textbf{Algorithm}&\textbf{Table Lookup}&\textbf{Linear}&\textbf{Non-Linear}\\
    \hline
    On-policy MC&YES&YES&YES \\
    \hline
    On-policy TD(0)&YES&YES&NO \\
    \hline
    On-policy TD($\lambda$)&YES&YES&NO \\
    \hline
    Off-policy MC&YES&YES&YES \\
    \hline
    Off-policy TD(0)&YES&NO&NO \\
    \hline
    Off-policy TD($\lambda$)&YES&NO&NO \\
    \hline
    \end{tabular}
    \item Gradient TD is an algorithm that fixes these issues and converges for all 3 situations
    \item Gradient TD follows the true gradient of the projected Bellman error unlike TD
    \item \textbf{Convergence of Control Algorithms} \\ \\
    \begin{tabular} {c | c | c | c}
      \textbf{Algorithm}&\textbf{Table Lookup}&\textbf{Linear}&\textbf{Non-linear} \\
      \hline
      MC Control&YES&(YES)&NO \\
      \hline
      Sarsa&YES&(YES)&NO \\
      \hline
      Q-learning&YES&NO&NO \\
      \hline
      Gradient Q-learning&YES&YES&NO 
    \end{tabular}
    \item \textbf{Batch Reinforcement Learning}
    \begin{itemize}
      \item Gradient descent is not sample efficient
      \item Batch methods seek to find the best fitting value function to the whole batch of training data
      \item \textbf{Least Squares Prediction}
      \begin{itemize}
        \item Use oracle data for value of each state $D = \{(s_1, v^\pi_1), \dots, (s_T, v^\pi_T)\}$
        \item Calculate least squares error as:
        \begin{gather*}
          LS(\textbf{w}) = \sum_{t=1}^T(v_t^\pi - \hat{v}(s_t, \textbf{w}))^2 \\
          = E_D[(v^\pi - \hat{v}(s, \textbf{w}))^2]
        \end{gather*}
      \end{itemize}
      \item \textbf{SGD with Experience Replay}
      \begin{itemize}
        \item Given an experience of data in the same format as $D$
        \item Repeat two steps:
        \item (1) Sample state, value from experience
        \begin{gather*} 
          (s, v^\pi) \sim D
        \end{gather*}
        \item (2) Apply SGD update:
        \begin{gather*}
          \Delta \textbf{w} = \alpha (v^\pi - \hat{v}(s, \textbf{w}))\nabla_{\textbf{w}}\hat{v}(s, \textbf{w})
        \end{gather*}
        \item This removes the correlation of regular SGD where each update was sequentially related to one another
        \item Another way to look at this is that we are storing our experienced data rather than throwing it away after each step
        \item This eventually converges to least squares solution
      \end{itemize}
      \item \textbf{Experience Replay in Deep-Q Networks}
      \begin{itemize}
        \item Take action $a_t$ according to $\epsilon$-greedy policy
        \item Store transition ($s_t, a_t, r_{t+1}, s_{t+1}$) in replay memory $D$
        \item Sample random mini-batch of transitions ($s, a, r, s'$) from $D$
        \item Compute Q-learning targets w.r.t old fixed parameters $\textbf{w}'$
        \item Optimize MSE between Q-network and Q-learning targets
        \begin{gather*}
          L_i(w_i) = E_{s, a, r, s' \sim D}[(r + \gamma \max\limits_{a'}Q(s', a'; \textbf{w}_i') - Q(s, a; \textbf{w}_i))^2]
        \end{gather*}
        \item Using variant of SGD
        \item This is stable vs. naive Q-learning because (1) it uses experience replay and (2) uses fixed Q-targets
        \item Experience replay stablizes function approx because it de-corelates the trajectories
        \item The fixed Q-targets are obtained by having two separate networks where the ``frozen'' one (not being updated) is used as the target to bootstrap to
      \end{itemize}
    \end{itemize}
    \item \textbf{On Policy Distribution}
    \begin{itemize}
      \item A distribution of time spent in each state (normalized to sum to one) can be calculated as
      \begin{gather*}
        \mu(s) = \frac{\eta(s)}{\sum_{s'}\eta(s')}, s \in S
      \end{gather*}
      \item Where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode
    \end{itemize}
    \item \textbf{Coarse Coding}
    \begin{itemize}
      \item Cover state space in overlapping spaces
      \item Features are then just binary (or non-binary) values indicating if the agent is in a particular space
      \item Since the spaces are overlapping, the features may have more than a single positive value
      \item The larger each individual space is then larger the update area will be
      \item While the size of each individual space causes generalizations to be larger/smaller,
      the final function learned is only slightly affected by the size of the spaces.
    \end{itemize}
    \item \textbf{Tile Coding}
    \begin{itemize}
      \item Form of coarse coding for multi-dimensional continuous spaces
      \item Uses multiple overlapping grid-tilings that are offset from one another by a fraction of tile width in each dimension
      \item Each state then falls into exactly one tile of each tiling
      \item Using asymmetric tiling offsets allows for better feature generalization
      \item Within small squares of size $\frac{w}{n}$ where $w$ is tile width and $n$ is number of tilings,
      all states activate the same tiles, have the same feature representation, and the same approximated value
      \item Often makes sense to use different shape tiles for different tilings
    \end{itemize}
    \item \textbf{Radial Basis Function}
    \begin{itemize}
      \item Generalization of coarse coding to continuous valued features
      \item Typical feature $x_i$ has Gaussian response $x_i(s)$ dependent only on the distance
      between the state, $s$, and the feature's prototypical or center state, $c_i$, relative to the
      feature's width, $\sigma_i$
      \begin{gather*}
        x_i(s) = \exp(-\frac{||s - c_i||^2}{2\sigma_i^2})
      \end{gather*}
    \end{itemize}
    \item Setting $\alpha$ to $(\tau E[x^\top x])^{-1}$ is a good rule of thumb
    \item Kernel trick
    \item \textbf{Interest and Emphasis}
    \begin{itemize}
      \item On-policy distribution is distribution of states encountered while following
      the target policy
      \item \textbf{Interest}: non-negative scaler measure $I_t$ that indicates the degree to which we
      are interested in accurately valuing the state (or state-action pair) at time $t$
      \item Distribution $\mu$ following target-policy is then weighted by $I$
      \item \textbf{Emphasis}: scaler $M_t$ that multiplies the learning update therefore emphasizing or de-emphasizing
      updates at time $t$
      \item Emphasis is determined recursively from the interest by:
      \begin{gather*}
        M_t = I_t + \gamma^nM_{t-n}
      \end{gather*}
    \end{itemize}
    \item Average reward defined as:
    \begin{gather*}
      r(\pi) = \sum\limits_{s}\mu_\pi(s)\sum\limits_{a}\pi(a|s)\sum\limits_{s', r}p(s', r | s, a)
    \end{gather*}
    \item In average reward setting, the returns are defined in terms of differences between
    rewards and the average reward (this is called \textit{differential return}):
    \begin{gather*}
      G_t = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \dots
    \end{gather*}
  \end{itemize}


\end{document}
